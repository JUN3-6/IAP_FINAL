{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN1NXPoUeo7w5hirKW79BNt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RgIjw3DR8AU_","executionInfo":{"status":"ok","timestamp":1750248562370,"user_tz":-540,"elapsed":23652,"user":{"displayName":"이재원","userId":"00961475728525731753"}},"outputId":"98c184fb-5e3f-4ea2-ccc5-5b2f23638d15"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["cd '/content/drive/MyDrive/IAP_Final/PC/source'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KuAY5XUt8BR1","executionInfo":{"status":"ok","timestamp":1750248565082,"user_tz":-540,"elapsed":682,"user":{"displayName":"이재원","userId":"00961475728525731753"}},"outputId":"285e8bc5-4ce3-418c-b782-f8e074e52660"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/10b_mTeA0LBn0XnIPn1kDV03ig94XXR0w/IAP_Final/PC/source\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class VGGish(nn.Module):\n","    def __init__(self):\n","        super(VGGish, self).__init__()\n","        self.features = nn.Sequential(\n","            # Conv Block 1\n","            nn.Conv2d(1, 64, kernel_size=3, padding=1), nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            # Conv Block 2\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            # Conv Block 3\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU(),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            # Conv Block 4\n","            nn.Conv2d(256, 512, kernel_size=3, padding=1), nn.ReLU(),\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(512 * 6 * 4, 4096), nn.ReLU(),\n","            nn.Linear(4096, 128)  # Output embedding size = 128\n","        )\n","\n","    def forward(self, x):  # x: [B, 1, 96, 64]\n","        x = self.features(x)\n","        x = self.classifier(x)\n","        return x\n"],"metadata":{"id":"8fcGJS6k5v2y"},"execution_count":null,"outputs":[]},{"source":["import torch\n","import torchaudio\n","import time\n","from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n","\n","# 🔁 VGGish 모델 클래스\n","# from your_code import VGGish\n","model = VGGish()\n","model.eval()\n","\n","# 🔁 오디오 전처리 함수 (96 x 64로 자르기 포함)\n","def preprocess_wav_to_vggish_input(wav_path):\n","    waveform, sr = torchaudio.load(wav_path)\n","\n","    # 리샘플링: 16kHz\n","    if sr != 16000:\n","        waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n","\n","    # 모노 변환\n","    if waveform.shape[0] > 1:\n","        waveform = waveform.mean(dim=0, keepdim=True)\n","\n","    # MelSpectrogram 생성 (VGGish 스펙과 유사하게 설정)\n","    mel_transform = MelSpectrogram(\n","        sample_rate=16000,\n","        n_fft=400,\n","        hop_length=160,\n","        win_length=400,\n","        n_mels=64,\n","        # Removed fmin and fmax as they are not supported by the constructor\n","        # fmin=125,\n","        # fmax=7500\n","    )\n","    db_transform = AmplitudeToDB()\n","\n","    mel_spec = mel_transform(waveform)\n","    log_mel_spec = db_transform(mel_spec)  # [1, 64, T]\n","\n","    # 96-frame (time axis) 고정\n","    if log_mel_spec.shape[2] < 96:\n","        # zero-padding\n","        pad = 96 - log_mel_spec.shape[2]\n","        log_mel_spec = torch.nn.functional.pad(log_mel_spec, (0, pad))\n","    else:\n","        log_mel_spec = log_mel_spec[:, :, :96]\n","\n","    # [1, 1, 96, 64] shape으로 변경\n","    log_mel_spec = log_mel_spec.permute(0, 2, 1)  # [1, 96, 64]\n","    log_mel_spec = log_mel_spec.unsqueeze(0)     # [1, 1, 96, 64]\n","\n","    return log_mel_spec\n","\n","# 🔁 Inference time 측정 함수\n","def measure_inference_time(wav_path, repeat=5):\n","    input_tensor = preprocess_wav_to_vggish_input(wav_path)\n","\n","    # warm-up\n","    with torch.no_grad():\n","        _ = model(input_tensor)\n","\n","    # 시간 측정\n","    times = []\n","    for _ in range(repeat):\n","        start = time.time()\n","        with torch.no_grad():\n","            _ = model(input_tensor)\n","        times.append(time.time() - start)\n","\n","    avg_time = sum(times) / repeat\n","    print(f\"Inference time for {wav_path}: {avg_time:.4f} seconds\")\n","    return avg_time\n","\n","# 🔁 실행 예시\n","wav_path = \"./voices/user_voice_clips/loaded_musics/cliped_user (1).wav\"\n","measure_inference_time(wav_path)"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wHvXBbvV8m4z","executionInfo":{"status":"ok","timestamp":1750248630982,"user_tz":-540,"elapsed":1741,"user":{"displayName":"이재원","userId":"00961475728525731753"}},"outputId":"9e41c1c4-c19c-4267-87a6-6f7e8e3970c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Inference time for ./voices/user_voice_clips/loaded_musics/cliped_user (1).wav: 0.0763 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["0.07627973556518555"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["pip install onnxruntime"],"metadata":{"id":"Mf1xMrxi5wAI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750248920850,"user_tz":-540,"elapsed":11210,"user":{"displayName":"이재원","userId":"00961475728525731753"}},"outputId":"5307672b-8eb0-463b-e098-5dc04e65a3fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting onnxruntime\n","  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n","Collecting coloredlogs (from onnxruntime)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n","Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (5.29.5)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n","Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\n","Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.22.0\n"]}]},{"cell_type":"code","source":["def measure_onnx_inference_time(wav_path, onnx_path, repeat=5):\n","    input_tensor = preprocess_wav_to_vggish_input(wav_path)  # [1, 1, 96, 64]\n","\n","    # 모델 기대 입력: [1, 64, 96]\n","    input_numpy = input_tensor.squeeze(1).permute(0, 2, 1).numpy().astype(np.float32)\n","\n","    ort_session = ort.InferenceSession(onnx_path, providers=[\"CPUExecutionProvider\"])\n","    input_name = ort_session.get_inputs()[0].name\n","\n","    # warm-up\n","    _ = ort_session.run(None, {input_name: input_numpy})\n","\n","    # 시간 측정\n","    times = []\n","    for _ in range(repeat):\n","        start = time.time()\n","        _ = ort_session.run(None, {input_name: input_numpy})\n","        times.append(time.time() - start)\n","\n","    avg_time = sum(times) / repeat\n","    print(f\"[ONNX] Inference time for {wav_path}: {avg_time:.4f} seconds\")\n","    return avg_time\n"],"metadata":{"id":"_QwR-ofS5wDg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["onnx_model_path = \"./model/VGGish/audioset-vggish-3.onnx\"\n","wav_path = \"./voices/user_voice_clips/loaded_musics/cliped_user (1).wav\"\n","\n","measure_onnx_inference_time(wav_path, onnx_model_path)"],"metadata":{"id":"LIApMlKj5wJN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750249169142,"user_tz":-540,"elapsed":2128,"user":{"displayName":"이재원","userId":"00961475728525731753"}},"outputId":"05dbfe91-a7ef-4b04-bd6f-cabf2a529e0d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[ONNX] Inference time for ./voices/user_voice_clips/loaded_musics/cliped_user (1).wav: 0.0459 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["0.04587597846984863"]},"metadata":{},"execution_count":17}]}]}